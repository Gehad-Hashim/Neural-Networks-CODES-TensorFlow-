{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48506360.0\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [1]----------- \"Our goal is to minimize loss function\"            <<traditional way>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "w1=tf.placeholder(tf.float32 , shape=(D,H))#w1 is weights\n",
    "w2=tf.placeholder(tf.float32 , shape=(H,D))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "diff =y_pred -y\n",
    "loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))# like ---> loss=( f(x)-y )^2 + b\n",
    "grad_w1 ,grad_w2=tf.gradients(loss,[w1,w2])\n",
    "#till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:\n",
    "    values={x:np.random.randn(N,D),\n",
    "           w1:np.random.randn(D,H),\n",
    "           w2:np.random.randn(H,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    out=sess.run(loss, feed_dict=values)\n",
    "    \n",
    "    loss=out\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51616696.0\n",
      "51616820.0\n",
      "51616950.0\n",
      "51617080.0\n",
      "51617210.0\n",
      "51617344.0\n",
      "51617470.0\n",
      "51617590.0\n",
      "51617720.0\n",
      "51617850.0\n",
      "51617976.0\n",
      "51618120.0\n",
      "51618240.0\n",
      "51618372.0\n",
      "51618500.0\n",
      "51618628.0\n",
      "51618760.0\n",
      "51618890.0\n",
      "51619016.0\n",
      "51619150.0\n",
      "51619270.0\n",
      "51619416.0\n",
      "51619544.0\n",
      "51619670.0\n",
      "51619804.0\n",
      "51619930.0\n",
      "51620060.0\n",
      "51620188.0\n",
      "51620320.0\n",
      "51620450.0\n",
      "51620576.0\n",
      "51620710.0\n",
      "51620840.0\n",
      "51620972.0\n",
      "51621104.0\n",
      "51621244.0\n",
      "51621372.0\n",
      "51621496.0\n",
      "51621630.0\n",
      "51621764.0\n",
      "51621890.0\n",
      "51622028.0\n",
      "51622150.0\n",
      "51622284.0\n",
      "51622412.0\n",
      "51622544.0\n",
      "51622680.0\n",
      "51622810.0\n",
      "51622944.0\n",
      "51623070.0\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [2]----------- \"Our goal is to minimize loss function\"               <<generate 'for' loop>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "w1=tf.placeholder(tf.float32 , shape=(D,H))#w1 is weights\n",
    "w2=tf.placeholder(tf.float32 , shape=(H,D))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "diff =y_pred -y\n",
    "loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))# like ---> loss=( f(x)-y )^2 + b\n",
    "grad_w1 ,grad_w2=tf.gradients(loss,[w1,w2])\n",
    "#till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:\n",
    "    values={x:np.random.randn(N,D),\n",
    "           w1:np.random.randn(D,H),\n",
    "           w2:np.random.randn(H,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    learning_rate=1e-5\n",
    "    for t in range (50):\n",
    "        out=sess.run(loss , feed_dict=values)\n",
    "        loss_val =out\n",
    "        values[w1] -=learning_rate* grad_w1_val\n",
    "        values[w2] -=learning_rate* grad_w2_val\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n",
      "49282052.0\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [3]----------- \"Our goal is to minimize loss function\"          <<wights are variables>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "w1=tf.Variable(tf.random_normal((D,H)))#w1 is weights\n",
    "w2=tf.Variable(tf.random_normal((H,D)))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "diff =y_pred -y\n",
    "loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))# like ---> loss=( f(x)-y )^2 + b\n",
    "grad_w1 ,grad_w2=tf.gradients(loss,[w1,w2])\n",
    "\n",
    "learning_rate=1e-5\n",
    "new_w1 =w1.assign(w1-learning_rate* grad_w1)\n",
    "new_w2 =w2.assign(w2-learning_rate* grad_w2)\n",
    "#till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:        #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val , =sess.run([loss], feed_dict=values)\n",
    "        print(loss_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49507896.0\n",
      "17590076.0\n",
      "9275882.0\n",
      "5475232.5\n",
      "3450659.0\n",
      "2271492.5\n",
      "1544766.5\n",
      "1076796.5\n",
      "765048.56\n",
      "551831.56\n",
      "402863.72\n",
      "296967.28\n",
      "220751.9\n",
      "165250.61\n",
      "124494.21\n",
      "94318.22\n",
      "71831.09\n",
      "54957.445\n",
      "42264.645\n",
      "32664.242\n",
      "25361.79\n",
      "19801.69\n",
      "15548.395\n",
      "12279.125\n",
      "9763.236\n",
      "7821.4487\n",
      "6318.339\n",
      "5151.8\n",
      "4244.6055\n",
      "3537.707\n",
      "2985.815\n",
      "2554.3042\n",
      "2216.2925\n",
      "1951.1812\n",
      "1742.9875\n",
      "1579.2257\n",
      "1450.3832\n",
      "1348.9275\n",
      "1268.9626\n",
      "1205.9019\n",
      "1156.1326\n",
      "1116.887\n",
      "1085.926\n",
      "1061.5596\n",
      "1042.3411\n",
      "1027.2166\n",
      "1015.2997\n",
      "1005.94696\n",
      "998.6145\n",
      "992.9048\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [4]----------- \"Our goal is to minimize loss function\"       <<use updates>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "w1=tf.Variable(tf.random_normal((D,H)))#w1 is weights\n",
    "w2=tf.Variable(tf.random_normal((H,D)))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "diff =y_pred -y\n",
    "loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))# like ---> loss=( f(x)-y )^2 + b\n",
    "grad_w1 ,grad_w2=tf.gradients(loss,[w1,w2])\n",
    "\n",
    "learning_rate=1e-5\n",
    "new_w1 =w1.assign(w1-learning_rate* grad_w1)\n",
    "new_w2 =w2.assign(w2-learning_rate* grad_w2)\n",
    "updates=tf.group(new_w1,new_w2)\n",
    "#till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:        #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val ,_ =sess.run([loss,updates], feed_dict=values)\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50756490.0\n",
      "17778724.0\n",
      "9246779.0\n",
      "5394226.0\n",
      "3371868.2\n",
      "2210008.5\n",
      "1501138.6\n",
      "1048397.75\n",
      "748203.2\n",
      "543905.8\n",
      "401013.0\n",
      "299202.38\n",
      "225356.81\n",
      "171185.72\n",
      "130989.5\n",
      "100959.336\n",
      "78143.945\n",
      "60880.18\n",
      "47702.383\n",
      "37551.023\n",
      "29706.654\n",
      "23622.0\n",
      "18876.297\n",
      "15156.931\n",
      "12234.923\n",
      "9931.703\n",
      "8108.744\n",
      "6666.0938\n",
      "5516.6123\n",
      "4604.976\n",
      "3881.14\n",
      "3304.7615\n",
      "2844.6323\n",
      "2476.5537\n",
      "2181.5818\n",
      "1945.0078\n",
      "1755.0511\n",
      "1602.3683\n",
      "1479.5088\n",
      "1380.5665\n",
      "1300.7976\n",
      "1236.4813\n",
      "1184.5955\n",
      "1142.7577\n",
      "1109.004\n",
      "1081.7927\n",
      "1059.8726\n",
      "1042.2109\n",
      "1027.9866\n",
      "1016.54645\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [5]----------- \"Our goal is to minimize loss function\"     <<use optimizer>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "w1=tf.Variable(tf.random_normal((D,H)))#w1 is weights\n",
    "w2=tf.Variable(tf.random_normal((H,D)))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "diff =y_pred -y\n",
    "loss=tf.reduce_mean(tf.reduce_sum(diff**2,axis=1))# like ---> loss=( f(x)-y )^2 +b\n",
    "optimizer=tf.train.GradientDescentOptimizer(1e-5) \n",
    "updates=optimizer.minimize(loss)          #till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:                  #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val ,_ =sess.run([loss,updates], feed_dict=values)\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52151.688\n",
      "52103.23\n",
      "52054.84\n",
      "52006.523\n",
      "51958.273\n",
      "51910.086\n",
      "51861.97\n",
      "51813.926\n",
      "51765.94\n",
      "51718.03\n",
      "51670.188\n",
      "51622.41\n",
      "51574.703\n",
      "51527.055\n",
      "51479.48\n",
      "51431.97\n",
      "51384.52\n",
      "51337.152\n",
      "51289.84\n",
      "51242.6\n",
      "51195.42\n",
      "51148.31\n",
      "51101.266\n",
      "51054.29\n",
      "51007.375\n",
      "50960.527\n",
      "50913.74\n",
      "50867.035\n",
      "50820.367\n",
      "50773.785\n",
      "50727.266\n",
      "50680.81\n",
      "50634.41\n",
      "50588.09\n",
      "50541.824\n",
      "50495.625\n",
      "50449.49\n",
      "50403.414\n",
      "50357.406\n",
      "50311.465\n",
      "50265.594\n",
      "50219.777\n",
      "50174.02\n",
      "50128.33\n",
      "50082.703\n",
      "50037.15\n",
      "49991.65\n",
      "49946.203\n",
      "49900.832\n",
      "49855.516\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [6]----------- \"Our goal is to minimize loss function\"     <<use predefined losses>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "w1=tf.Variable(tf.random_normal((D,H)))#w1 is weights\n",
    "w2=tf.Variable(tf.random_normal((H,D)))#w2 is weights\n",
    "\n",
    "h=tf.maximum(tf.matmul(x,w1),0)#h is hidden layer\n",
    "y_pred=tf.matmul(h,w2)# y_pred is output layer\n",
    "\n",
    "loss=tf.losses.mean_squared_error(y_pred,y)\n",
    "optimizer=tf.train.GradientDescentOptimizer(1e-5) \n",
    "updates=optimizer.minimize(loss)          #till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:                  #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val ,_ =sess.run([loss,updates], feed_dict=values)\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028430A87888>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028430A87888>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028430A87888>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000028430A87888>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002843092F5C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002843092F5C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002843092F5C8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002843092F5C8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "2.0070264\n",
      "2.0070255\n",
      "2.0070245\n",
      "2.0070233\n",
      "2.0070224\n",
      "2.0070214\n",
      "2.0070205\n",
      "2.0070195\n",
      "2.0070186\n",
      "2.0070176\n",
      "2.0070167\n",
      "2.0070157\n",
      "2.0070148\n",
      "2.0070136\n",
      "2.0070126\n",
      "2.0070117\n",
      "2.0070107\n",
      "2.0070097\n",
      "2.0070088\n",
      "2.0070078\n",
      "2.007007\n",
      "2.007006\n",
      "2.007005\n",
      "2.007004\n",
      "2.0070028\n",
      "2.0070019\n",
      "2.007001\n",
      "2.007\n",
      "2.0069988\n",
      "2.0069978\n",
      "2.006997\n",
      "2.0069962\n",
      "2.0069952\n",
      "2.0069942\n",
      "2.006993\n",
      "2.006992\n",
      "2.0069911\n",
      "2.0069902\n",
      "2.0069892\n",
      "2.0069883\n",
      "2.006987\n",
      "2.0069861\n",
      "2.0069852\n",
      "2.0069842\n",
      "2.006983\n",
      "2.006982\n",
      "2.0069811\n",
      "2.0069802\n",
      "2.0069792\n",
      "2.0069783\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [7]----------- \"Our goal is to minimize loss function\"     <<use layers dense>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "\n",
    "init=tf.variance_scaling_initializer(2.0)\n",
    "h=tf.layers.dense(inputs=x,units=H,activation=tf.nn.relu,kernel_initializer=init)#h is hidden layer\n",
    "y_pred=tf.layers.dense(inputs=x,units=D,activation=tf.nn.relu,kernel_initializer=init)# y_pred is output layer\n",
    "\n",
    "loss=tf.losses.mean_squared_error(y_pred,y)\n",
    "optimizer=tf.train.GradientDescentOptimizer(1e-5) \n",
    "updates=optimizer.minimize(loss)          #till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:                  #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val ,_ =sess.run([loss,updates], feed_dict=values)\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\lenovo\\Anaconda3\\envs\\NNlab2020\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "1.1635964\n",
      "1.163596\n",
      "1.1635957\n",
      "1.1635952\n",
      "1.163595\n",
      "1.1635945\n",
      "1.1635942\n",
      "1.163594\n",
      "1.1635935\n",
      "1.163593\n",
      "1.1635928\n",
      "1.1635925\n",
      "1.1635921\n",
      "1.1635915\n",
      "1.1635913\n",
      "1.1635909\n",
      "1.1635906\n",
      "1.1635902\n",
      "1.1635898\n",
      "1.1635894\n",
      "1.163589\n",
      "1.1635886\n",
      "1.1635883\n",
      "1.1635879\n",
      "1.1635875\n",
      "1.1635872\n",
      "1.1635866\n",
      "1.1635864\n",
      "1.1635859\n",
      "1.1635857\n",
      "1.1635852\n",
      "1.1635847\n",
      "1.1635845\n",
      "1.1635841\n",
      "1.1635838\n",
      "1.1635834\n",
      "1.163583\n",
      "1.1635827\n",
      "1.1635823\n",
      "1.1635817\n",
      "1.1635815\n",
      "1.1635811\n",
      "1.1635808\n",
      "1.1635803\n",
      "1.1635801\n",
      "1.1635796\n",
      "1.1635793\n",
      "1.1635789\n",
      "1.1635786\n",
      "1.1635782\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [8]----------- \"Our goal is to minimize loss function\"     << Keras>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "x=tf.placeholder(tf.float32 , shape=(N,D))#x is input layer\n",
    "y=tf.placeholder(tf.float32 , shape=(N,D))#y is Exact\n",
    "\n",
    "\n",
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense( H,input_shape=(D,),activation=tf.nn.relu))#h is hidden layer\n",
    "model.add(tf.keras.layers.Dense( (D) ))# y_pred is output layer\n",
    "y_pred=model(x)\n",
    "          \n",
    "loss=tf.losses.mean_squared_error(y_pred,y)\n",
    "optimizer=tf.train.GradientDescentOptimizer(1e-5) \n",
    "updates=optimizer.minimize(loss)          #till now ONLY the computational graph is built\n",
    "with tf.Session() as sess:                  #--------NOW the Excution\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    values={x:np.random.randn(N,D),\n",
    "           y:np.random.randn(N,D)}\n",
    "    for t in range (50):\n",
    "        loss_val ,_ =sess.run([loss,updates], feed_dict=values)\n",
    "        print(loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "64/64 [==============================] - 1s 10ms/sample - loss: 1.1625\n",
      "Epoch 2/50\n",
      "64/64 [==============================] - 0s 153us/sample - loss: 1.1264\n",
      "Epoch 3/50\n",
      "64/64 [==============================] - 0s 142us/sample - loss: 1.0980\n",
      "Epoch 4/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 1.0752\n",
      "Epoch 5/50\n",
      "64/64 [==============================] - 0s 157us/sample - loss: 1.0564\n",
      "Epoch 6/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 1.0407\n",
      "Epoch 7/50\n",
      "64/64 [==============================] - 0s 423us/sample - loss: 1.0275\n",
      "Epoch 8/50\n",
      "64/64 [==============================] - 0s 242us/sample - loss: 1.0162\n",
      "Epoch 9/50\n",
      "64/64 [==============================] - 0s 212us/sample - loss: 1.0066\n",
      "Epoch 10/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9982\n",
      "Epoch 11/50\n",
      "64/64 [==============================] - 0s 94us/sample - loss: 0.9908\n",
      "Epoch 12/50\n",
      "64/64 [==============================] - 0s 266us/sample - loss: 0.9843\n",
      "Epoch 13/50\n",
      "64/64 [==============================] - 0s 110us/sample - loss: 0.9786\n",
      "Epoch 14/50\n",
      "64/64 [==============================] - 0s 110us/sample - loss: 0.9734\n",
      "Epoch 15/50\n",
      "64/64 [==============================] - 0s 110us/sample - loss: 0.9688\n",
      "Epoch 16/50\n",
      "64/64 [==============================] - 0s 110us/sample - loss: 0.9646\n",
      "Epoch 17/50\n",
      "64/64 [==============================] - 0s 235us/sample - loss: 0.9607\n",
      "Epoch 18/50\n",
      "64/64 [==============================] - 0s 556us/sample - loss: 0.9572\n",
      "Epoch 19/50\n",
      "64/64 [==============================] - 0s 266us/sample - loss: 0.9538\n",
      "Epoch 20/50\n",
      "64/64 [==============================] - 0s 423us/sample - loss: 0.9507\n",
      "Epoch 21/50\n",
      "64/64 [==============================] - 0s 243us/sample - loss: 0.9478\n",
      "Epoch 22/50\n",
      "64/64 [==============================] - 0s 235us/sample - loss: 0.9450\n",
      "Epoch 23/50\n",
      "64/64 [==============================] - 0s 235us/sample - loss: 0.9423\n",
      "Epoch 24/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9397\n",
      "Epoch 25/50\n",
      "64/64 [==============================] - 0s 250us/sample - loss: 0.9372\n",
      "Epoch 26/50\n",
      "64/64 [==============================] - 0s 172us/sample - loss: 0.9348\n",
      "Epoch 27/50\n",
      "64/64 [==============================] - 0s 141us/sample - loss: 0.9324\n",
      "Epoch 28/50\n",
      "64/64 [==============================] - 0s 102us/sample - loss: 0.9301\n",
      "Epoch 29/50\n",
      "64/64 [==============================] - 0s 212us/sample - loss: 0.9277\n",
      "Epoch 30/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9255\n",
      "Epoch 31/50\n",
      "64/64 [==============================] - 0s 125us/sample - loss: 0.9232\n",
      "Epoch 32/50\n",
      "64/64 [==============================] - 0s 219us/sample - loss: 0.9209\n",
      "Epoch 33/50\n",
      "64/64 [==============================] - 0s 204us/sample - loss: 0.9186\n",
      "Epoch 34/50\n",
      "64/64 [==============================] - 0s 156us/sample - loss: 0.9164\n",
      "Epoch 35/50\n",
      "64/64 [==============================] - 0s 219us/sample - loss: 0.9141\n",
      "Epoch 36/50\n",
      "64/64 [==============================] - 0s 228us/sample - loss: 0.9118\n",
      "Epoch 37/50\n",
      "64/64 [==============================] - 0s 188us/sample - loss: 0.9095\n",
      "Epoch 38/50\n",
      "64/64 [==============================] - 0s 157us/sample - loss: 0.9071\n",
      "Epoch 39/50\n",
      "64/64 [==============================] - 0s 501us/sample - loss: 0.9048\n",
      "Epoch 40/50\n",
      "64/64 [==============================] - 0s 877us/sample - loss: 0.9024\n",
      "Epoch 41/50\n",
      "64/64 [==============================] - 0s 423us/sample - loss: 0.9000\n",
      "Epoch 42/50\n",
      "64/64 [==============================] - 0s 439us/sample - loss: 0.8976\n",
      "Epoch 43/50\n",
      "64/64 [==============================] - 0s 306us/sample - loss: 0.8951\n",
      "Epoch 44/50\n",
      "64/64 [==============================] - 0s 298us/sample - loss: 0.8926\n",
      "Epoch 45/50\n",
      "64/64 [==============================] - 0s 1ms/sample - loss: 0.8901\n",
      "Epoch 46/50\n",
      "64/64 [==============================] - 0s 611us/sample - loss: 0.8875\n",
      "Epoch 47/50\n",
      "64/64 [==============================] - 0s 768us/sample - loss: 0.8850\n",
      "Epoch 48/50\n",
      "64/64 [==============================] - 0s 251us/sample - loss: 0.8823\n",
      "Epoch 49/50\n",
      "64/64 [==============================] - 0s 407us/sample - loss: 0.8797\n",
      "Epoch 50/50\n",
      "64/64 [==============================] - 0s 407us/sample - loss: 0.8770\n"
     ]
    }
   ],
   "source": [
    "#------------CODE [9]----------- \"Our goal is to minimize loss function\"     << Keras>>\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "N,D,H=64,1000,100\n",
    "\n",
    "\n",
    "model=tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense( H,input_shape=(D,),activation=tf.nn.relu))#h is hidden layer\n",
    "model.add(tf.keras.layers.Dense( (D) ))# y_pred is output layer\n",
    " \n",
    "model.compile(loss=tf.keras.losses.mean_squared_error ,optimizer=tf.keras.optimizers.SGD(lr=1e0))        \n",
    "\n",
    "x=np.random.randn(N,D)\n",
    "y=np.random.randn(N,D)\n",
    "history=model.fit(x,y,epochs=50,batch_size=N )        \n",
    " #Keras can handle thetraining loop for you! No sessions or feed_dict    !!!!Attention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NNlab2020",
   "language": "python",
   "name": "nnlab2020"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
